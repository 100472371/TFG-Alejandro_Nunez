@inproceedings{10.5555/3721488.3721842,
author = {Lemaignan, S\'{e}verin and Ferrini, Lorenzo and Gebelli, Ferran and Ros, Raquel and Juri?i\'{c}, Luka and Cooper, Sara},
title = {Hands-on: From Zero to an Interactive Social Robot using ROS4HRI and LLMs},
year = {2025},
publisher = {IEEE Press},
abstract = {This tutorial aims at providing an up-to-date picture of the state-of-art regarding using the Robot Operating System (ROS) to build robots with socio-cognitive capabilities.The tutorial will briefly introduce the ROS4HRI framework, and show how it can be used to build a complete social robot architecture, from human perception to expressive social interaction.We will illustrate the full software integration required to implement an autonomous social robot using a combination of open-source ROS-based social perception modules, a semantic knowledge base and a Large Language Model (LLM).Participants will be able to follow along using a simple social interaction simulator, as well as their own webcams. The organisers will also provide a new PAL Robotics TIAGo Pro stand-alone head to demonstrate the same system running on actual hardware.},
booktitle = {Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {2009–2010},
numpages = {2},
keywords = {large language models, ros, ros 2, ros4hri, social robotics},
location = {Melbourne, Australia},
series = {HRI '25}
}

@article{10.1145/3725840,
author = {Lopez-Cardona, Angela and Idesis, Sebastian and Barreda-\'{A}ngeles, Miguel and Abadal, Sergi and Arapakis, Ioannis},
title = {OASST-ETC Dataset: Alignment Signals from Eye-tracking Analysis of LLM Responses},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {3},
url = {https://doi.org/10.1145/3725840},
doi = {10.1145/3725840},
abstract = {While Large Language Models (LLMs) have significantly advanced natural language processing, aligning them with human preferences remains an open challenge. Although current alignment methods rely primarily on explicit feedback, eye-tracking (ET) data offers insights into real-time cognitive processing during reading. In this paper, we present OASST-ETC, a novel eye-tracking corpus capturing reading patterns from 24 participants, while evaluating LLM-generated responses from the OASST1 dataset. Our analysis reveals distinct reading patterns between preferred and non-preferred responses, which we compare with synthetic eye-tracking data. Furthermore, we examine the correlation between human reading measures and attention patterns from various transformer-based models, discovering stronger correlations in preferred responses. This work introduces a unique resource for studying human cognitive processing in LLM evaluation and suggests promising directions for incorporating eye-tracking data into alignment methods. The dataset and analysis code are publicly available.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = may,
articleno = {ETRA15},
numpages = {29},
keywords = {LLMs, RLHF, eye-tracking}
}

@inproceedings{10.5555/3721488.3721763,
author = {Cooper, Sara and Ros, Raquel and Lemaignan, S\'{e}verin and Gebell\'{\i}, Ferran and Ferrini, Lorenzo and Juri?i\'{c}, Luka},
title = {Demonstration of an Open-source ROS 2 Framework and Simulator for Situated Interactive Social Robot},
year = {2025},
publisher = {IEEE Press},
abstract = {We introduce an open-source ROS 2 architecture for situated social robots, along with a simulator that allows mixed-reality development and interactions. The architecture is a hybrid symbolic/subsymbolic system that integrates explicit ontology semantics for perception, reasoning, and execution, with LLMs. It features multimodal social perception by leveraging the open source ROS4HRI framework; LLMs (both edge- and cloud-based) to facilitate natural language interaction between the user and system; KnowledgeCore, an open-source knowledge base, to reason about facts in the world; and an intent-based controller to supervise the execution of parallel/sequential tasks and skills. We demonstrate our system architecture with a social robot running the mixed-reality system.},
booktitle = {Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {1770–1772},
numpages = {3},
keywords = {mixed-reality simulator, ros 2 framework, situated social robots},
location = {Melbourne, Australia},
series = {HRI '25}
}

@inproceedings{10.1145/3708319.3733652,
author = {Portaz, Miguel and Manjarr\'{e}s, Angeles and Santos, Olga C. and Cabestrero, Ra\'{u}l and Quir\'{o}s, Pilar and Hermosilla, Mar and Puertas-Ramirez, David and Boticario, Jesus G. and Lucas P\'{e}rez, Gadea and Serrano-Mamolar, Ana and Arnaiz-Gonz\'{a}lez, \'{A}lvar and Arevalillo-Herr\'{a}ez, Miguel and Arnau, David and Arnau-Gonz\'{a}lez, Pablo and Fern\'{a}ndez-Matell\'{a}n, Ra\'{u}l and Martin Gomez, David},
title = {Developing Human-Centered Intelligent Learning Systems: the application of CARAIX framework},
year = {2025},
isbn = {9798400713996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708319.3733652},
doi = {10.1145/3708319.3733652},
abstract = {Developing personalized systems requires architectures that ensure adaptability, explainability, and ethical compliance while maintaining user engagement and trust. To assess whether a system meet these principles, this article puts to the test a novel framework, denominated CARAIX (Collaborative, Adaptive, and Responsible Artificial Intelligence -AI- assisted by eXplainability), designed to develop intelligent systems with a human-centered approach, to support real-time feedback and bias-aware AI decision-making. CARAIX is inspired by the principles of the Hybrid Intelligence (HI) paradigm and emphasizes the integration of explainable AI techniques in the development process to enhance user interaction and system reliability. This paper analyses, using a peer-validated rubric, how the dimensions of the HI paradigm are integrated across four diverse and real-world learning scenarios, including intelligent tutoring systems, psychomotor skill acquisition, autonomous driving training, and acquiring occupational safety competences. CARAIX is designed for scalability and reuse, facilitating integration into various AI-driven educational domains. We aim to share its potential for sustainable and ethically sound AI-enhanced multidisciplinary learning environments and for the assessment of whether a system complies with HI principles.},
booktitle = {Adjunct Proceedings of the 33rd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {177–186},
numpages = {10},
keywords = {User Modeling, Human-Centered Systems, Hybrid Intelligence, Wearables, ITS, Framework, Virtual Reality},
location = {
},
series = {UMAP Adjunct '25}
}

@inproceedings{10.5555/3721488.3721668,
author = {Gebell\'{\i}, Ferran and Hriscu, Lavinia and Ros, Raquel and Lemaignan, S\'{e}verin and Sanfeliu, Alberto and Garrell, Ana\'{\i}s},
title = {Personalised Explainable Robots Using LLMs},
year = {2025},
publisher = {IEEE Press},
abstract = {In the field of Human-Robot Interaction (HRI), a key challenge lies in enabling humans to comprehend the decisions and behaviours of robots. One promising approach involves leveraging Theory of Mind (ToM) frameworks, wherein a robot estimates the mental model that a user holds about its functioning and compares this with the representation of its internal mental model. This comparison allows the robot to identify potential mismatches and generate communicative actions to bridge such gaps. Effective communication requires the robot to maintain unique mental models for each user and personalise explanations based on past interactions. To address this, we propose an architecture grounded in Large Language Models (LLMs) that operationalises this theoretical framework. We demonstrate the feasibility of this approach through qualitative examples, showcasing responses provided by a robot patrolling a geriatric hospital.},
booktitle = {Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {1304–1308},
numpages = {5},
keywords = {explainability, llm, personalisation, xhri},
location = {Melbourne, Australia},
series = {HRI '25}
}

@inproceedings{10.1145/3706370.3731642,
author = {Montagud Climent, Mario and Benavente, \'{A}lvaro Egea and Martos Cabr\'{e}, Marc and Montesa, Javier and Iba\~{n}ez, Francisco and Fern\'{a}ndez, Sergi},
title = {Social eXtended Reality (XR) and Virtual Production: Toward New Engaging Immersive Experiences},
year = {2025},
isbn = {9798400713910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706370.3731642},
doi = {10.1145/3706370.3731642},
abstract = {Social eXtended Reality (XR) is poised to become a dominant medium for remote communication, social interaction, and collaboration in the near future. However, its potential can be significantly magnified by integrating additional technological enablers. On the one hand, the support for realistic and volumetric holographic representations of users, captured in real-time via affordable setups, will result in enhanced levels of quality of interaction, co-presence and trustworthiness compared to using synthetic avatar-based user representation formats. On the other hand, the availability of high-quality multi-modal virtual production tools will enrich the immersion, interaction and storytelling possibilities, while it will allow providing such engaging services to large-scale audiences via 2D video distribution platforms. This paper presents a strategic vision toward achieving a modular and seamless integration between innovative Social XR, holographic communication, and virtual production tools to exploit all such potential advantages. In particular, it proposes a high-level architectural framework to cohesively integrate such technological enablers, accommodating novel features to meet newly derived requirements. Then, it outlines potential applicability scenarios and reports preliminary results as initial evidence of the full achievable potential.},
booktitle = {Proceedings of the 2025 ACM International Conference on Interactive Media Experiences},
pages = {458–461},
numpages = {4},
keywords = {Holographic Communications, Holoportation, Multimodal Interaction, Social eXtended Reality (XR), Virtual Production, Volumetric Video},
location = {
},
series = {IMX '25}
}

@inproceedings{10.1145/3706598.3714315,
author = {Bouzbib, Elodie and Sarasate, Iosune and Fern\'{a}ndez, Unai Javier and Fern\'{a}ndez, Ivan and Lopez-Amo, Manuel and Ezcurdia, I\~{n}igo and Marzo, Asier},
title = {FlexiVol: a Volumetric Display with an Elastic Diffuser to Enable Reach-Through Interaction},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714315},
doi = {10.1145/3706598.3714315},
abstract = {Volumetric displays render true 3D graphics without forcing users to wear headsets or glasses. However, the optical diffusers that volumetric displays employ are rigid and thus do not allow for direct interaction. FlexiVol employs elastic diffusers to allow users to reach inside the display volume to have direct interaction with true 3D content. We explored various diffuser materials in terms of visual and mechanical properties. We correct the distortions of the volumetric graphics projected on elastic oscillating diffusers and propose a design space for FlexiVol, enabling various gestures and actions through direct interaction techniques. A user study suggests that selection, docking and tracing tasks can be performed faster and more precisely using direct interaction when compared to indirect interaction with a 3D mouse. Finally, applications such as a virtual pet or landscape edition highlight the advantages of a volumetric display that supports direct interaction.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {977},
numpages = {16},
keywords = {Volumetric Displays, Direct Interaction, True 3D graphics, Flexible Diffuser, Projection},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3706370.3731707,
author = {Goyena, Marta and Dal Magro, Matteo and Merolli, Martina and Barbero Garc\'{\i}a, David and Cort\'{e}s, Carlos and Orduna, Marta and Fern\'{a}ndez-Alcaide, Ainhoa and Nava-Ruiz, Mar\'{\i}a and Guti\'{e}rrez, Jesus and Perez, Pablo and Garc\'{\i}a, Narciso},
title = {Immersive Cognitive Training for Job Integration with people with Intellectual Disabilities},
year = {2025},
isbn = {9798400713910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706370.3731707},
doi = {10.1145/3706370.3731707},
abstract = {The combination of eXtended Reality (XR) technologies with physiological signals offers innovative approaches to cognitive training for individuals with intellectual disabilities. This study presents a tool designed for cognitive training aimed at job integration through interactive and immersive environments. The tool was developed in collaboration with therapists from the Juan XXIII Foundation, an occupational center, and focuses on enhancing cognitive skills through serious games for job training. These games simulate realistic environments, such as a cafeteria and a supermarket, where users must complete various tasks. These tasks are designed to progressively increase in complexity as users advance, allowing them to improve their skills. Therapists can monitor and control the training sessions in real-time through an application, while users engage with the immersive scenarios. Additionally, the system incorporates non-invasive biosensors to track physiological data, such as heart rate, eye movement, etc. providing valuable insights into the emotional and cognitive states of users during the sessions. This approach enables a more comprehensive assessment of progress and well-being. This approach offers an innovative solution that highlights the potential of XR technologies in cognitive training and job placement for people with intellectual disabilities. A demo video can be seen in https://www.youtube.com/watch?v=3fdXp8vJkhQ},
booktitle = {Proceedings of the 2025 ACM International Conference on Interactive Media Experiences},
pages = {346–349},
numpages = {4},
keywords = {eXtended Reality, Accessibility, Cognitive skills, Intellectual disability, User assessment},
location = {
},
series = {IMX '25}
}

@inproceedings{10.1145/3706370.3727851,
author = {Goyena, Marta and Cort\'{e}s, Carlos and Orduna, Marta and Dal Magro, Matteo and Fern\'{a}ndez-Alcaide, Ainhoa and Nava-Ruiz, Mar\'{\i}a and Guti\'{e}rrez, Jes\'{u}s and Perez, Pablo and Garc\'{\i}a, Narciso},
title = {A Study on Immersive Behavioral Therapy for Individuals with Intellectual Disabilities with Fear of Stairs},
year = {2025},
isbn = {9798400713910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706370.3727851},
doi = {10.1145/3706370.3727851},
abstract = {The increasing deployment of immersive technologies is opening up opportunities in the field of behavioral therapies. In particular, the use of eXtended Reality technologies allows the development of therapies that require bringing users to a remote location without the need of going to the therapy site. This is especially useful when the therapy is aimed for phobia treatment. Specifically, in this paper we present a complete pilot study of an immersive therapy based on the systematic desensitization technique focused on the treatment of phobias to stairs for individuals with intellectual disabilities. The study involved: 1) the selection, classification, and creation of the immersive stimuli; 2) the development of an immersive tool for stimuli visualization and a tool for the therapists to monitor and control the sessions; 3) the design of a methodology for the assessment of the user experience, including biosensors and adapted questionnaires, scales and scoring to evaluate factors as discomfort, fear level and technical aspects of the system; 4) carrying out the therapy study with ten users during six months; and 5) the analysis of the results showing the benefits of the immersive therapy. The therapists were involved in all these steps guaranteeing the ecological validity of the study. The results show that the immersive therapy helps to overcome the fear of stairs. In addition, the success and holistic approach of this study will encourage and guide following studies of immersive therapies.},
booktitle = {Proceedings of the 2025 ACM International Conference on Interactive Media Experiences},
pages = {126–138},
numpages = {13},
keywords = {eXtended Reality, Accessibility, Intellectual disability, User assessment},
location = {
},
series = {IMX '25}
}

@inproceedings{10.5555/3721488.3721611,
author = {Ferrini, Lorenzo and Lemaignan, S\'{e}verin},
title = {VDB-based Spatially Grounded Semantics for Interactive Robots},
year = {2025},
publisher = {IEEE Press},
abstract = {This paper presents a new approach for representing spatially-grounded semantics in interactive robots. The method combines spatial and symbolic data to improve robot interactions in human-occupied environments. A key feature is a voxel-based data structure optimized for dynamic and sparse information, along with a global lookup table to manage and track spatially-grounded entities and their relationships. The implementation, which is integrated into a ROS 2-based framework, allows for seamless querying through semantic web APIs such as SPARQL. Initial tests demonstrate the efficiency of this system in supporting advanced scenarios in human-robot interaction. All the repositories developed as part of this contribution can be found at github.com/RepresentationMaps.},
booktitle = {Proceedings of the 2025 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {1005–1009},
numpages = {5},
keywords = {human-robot interaction, interactive robots, knowledge representation, semantic mapping},
location = {Melbourne, Australia},
series = {HRI '25}
}

@inproceedings{10.1145/3737821.3748525,
author = {Salinas-Bueno, Iosune and Mart\'{\i}nez-Bueso, Pau and Arb\'{o}s-Berenguer, Maria Teresa and Roig-Maim\'{o}, Maria Francesca and Mas-Sans\'{o}, Ramon},
title = {RehbeCa in Action! A Demo of Innovative Mobile Cervical Rehabilitation},
year = {2025},
isbn = {9798400719707},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3737821.3748525},
doi = {10.1145/3737821.3748525},
abstract = {Patients with cervical pain are often referred for rehabilitation treatment with a physiotherapist. This in-clinic treatment is usually combined with prescribed therapeutic exercises to perform at home. The dedication to the prescribed Home Exercise Program (HEP) is crucial to the effectiveness of the treatment and its beneficial long-term effects. However, involvement in HEP is usually low mainly due to lack of motivation and the feeling of external supervision. To overcome this, we propose RehbeCa, a complete functional prototype. It is a platform consisting of a mobile application (for patients) and a web application (for physical therapists). This allows the customization of HEP according to the needs of the patients. These new interaction techniques to monitor the HEP also ensure its proper performance. This maintains closer oversight and adjusts treatment as necessary. The mobile application consists mainly of a serious game designed to encourage patients to perform neck therapeutic exercises. This introduces a more appealing experience for patients. The exergame innovates the HEP integrating a camera-based head-tracker. Therefore, the interaction with the mobile application is based on the movement of the head (no additional sensors needed). This allows patients to perform their prescribed HEP on their own mobile devices, transforming rehabilitation into an innovative mobile experience: HEP anywhere and anytime.},
booktitle = {Adjunct Proceedings of the 27th International Conference on Mobile Human-Computer Interaction},
articleno = {25},
numpages = {5},
keywords = {Cervical rehabilitation, mobile devices, mHealth, head-tracker, physiotherapy, serious game},
location = {
},
series = {MobileHCI '25 Adjunct}
}

@inproceedings{10.1145/3708319.3734180,
author = {Perez-Martinez, Roberto and Casas-Ortiz, Alberto and Santos, Olga C.},
title = {MoRTELaban: a Neurosymbolic Framework for Motion Representation and Analysis based on Labanotation and Laban Movement Analysis},
year = {2025},
isbn = {9798400713996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708319.3734180},
doi = {10.1145/3708319.3734180},
abstract = {Human motion cannot be fully modeled by subsymbolic representations. While these extract precise hidden patterns in motion data, they are often task-specific and lack a semantic understatement of motion. Symbolic systems that mirror human cognition and explicit expressive processes are necessary for richer motion synthesis and analysis, enabling physical reasoning and expert knowledge encoding. In this work, we propose a neurosymbolic framework that combines Labanotation and Laban Movement Analysis (LMA), originally developed for dance, to represent and analyze human motion symbolically. We expand the existing LabanEditor to support full-body annotation and integrate it with AMASS, Mediapipe, and Kinect inputs through a SMPL-based format. Our system supports automatic annotation for the local functional and expressive aspects of motion, and enables bidirectional conversion between symbols and motion. While still a work in progress, this framework lays the groundwork for explainable, expressive motion modeling that can support human-robot interaction, motion preservation, and psychomotor learning systems.},
booktitle = {Adjunct Proceedings of the 33rd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {353–359},
numpages = {7},
keywords = {motion modeling, movement modeling, knowledge representation, Labanotation, LMA, expert systems},
location = {
},
series = {UMAP Adjunct '25}
}

@inproceedings{10.1145/3708319.3733657,
author = {Perez-Martinez, Roberto and Casas-Ortiz, Alberto and Santos, Olga C.},
title = {Towards Cultural Preservation of Traditional Motion Knowledge through Automated Annotations with MoRTELaban},
year = {2025},
isbn = {9798400713996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708319.3733657},
doi = {10.1145/3708319.3733657},
abstract = {Movement disciplines like dance or martial arts are carriers of cultural knowledge, identity, and tradition. However, oral traditions and video recordings make the preservation of this knowledge susceptible to being lost. Expert movement notation, in turn, holds the potential for precise capture and knowledge inheritance. However, motion notation approaches are not widespread, the process is often time-consuming, and the movements are hard to visualize without expert knowledge. In this work, we use Labanotation and Laban Movement Analysis (LMA), a notation system and method originally developed for dance, as a symbolic, interpretable framework for motion representation and preservation. Our contribution resides in the expansion of an existing annotation system, the LabanEditor, to handle full-body motion and data from multiple sources, and support the work of experts in annotating the movements. Our development, called MoRTELaban, supports motion-to-notation and inverse mapping from notation to keyframes, enabling exchange between video, motion capture, and Labanotation formats. This allows for the documentation and reconstruction of traditional motion practices using expert-readable scores and 3D skeletons.},
booktitle = {Adjunct Proceedings of the 33rd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {459–463},
numpages = {5},
keywords = {motion modeling, movement modeling, knowledge representation, Labanotation, Laban Movement Analysis (LMA), expert systems},
location = {
},
series = {UMAP Adjunct '25}
}

@inproceedings{10.1145/3706370.3727852,
author = {Cort\'{e}s, Carlos and Barbero Garc\'{\i}a, David and Guti\'{e}rrez, Jesus and Garc\'{\i}a, Narciso},
title = {The Impact of Segmentation Methods for Avatar representation on User Experience in a Task-Based XR experience},
year = {2025},
isbn = {9798400713910},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706370.3727852},
doi = {10.1145/3706370.3727852},
abstract = {As eXtended Reality (XR) continues to expand in applications ranging from gaming to education and training, enhancing user Quality of Experience (QoE) remains a critical focus. A key factor in XR environments is how users perceive their own bodies during interactive tasks, such as object manipulation and navigation. The accuracy and fidelity of these self-representations can significantly affect immersion, realism, and overall user engagement. This paper explores the influence of egocentric segmentation techniques on user experience within XR. Using the Quest 3 headset, we developed immersive experiences where users interact with virtual elements through representations of their hands, arms, and upper body. Participants performed two interactive tasks under three hand visualization conditions. While the visualization differed in levels of detail and fidelity, the tasks were either static or translational based. Subjective assessments measured global and visual QoE, sense of presence and perceived performance level across these conditions. All software developed is publicly available, ensuring transparency and reproducibility. While no significant differences were observed in overall subjective measurements across representation variations, users reported heightened immersion for all conditions; thus, showing the validity of these methods to be able to blend physical and virtual realities without degrading the experience. However, the results indicate that the difference between purely virtual environment representations together with a photorealistic body representation can affect the consistency of the experience. In particular, the involvement factor for the translational task suggests degradation when the whole arm is not shown (only the hand) and occlusions are not well managed, so that manual interactions based on photorealism do not meet expectations. Furthermore, visual results suggest difference between virtual and photorealistic hands, possibly due to their contrast with a non-realistic environment. Future research should focus on improving the fidelity of the virtual environments so body representation matches the representation of the environment.},
booktitle = {Proceedings of the 2025 ACM International Conference on Interactive Media Experiences},
pages = {175–186},
numpages = {12},
keywords = {Human-centered computing, User studies, Systems and tools for interaction design, Computing methodologies, Mixed / augmented reality},
location = {
},
series = {IMX '25}
}

@inproceedings{10.1145/3715336.3735730,
author = {Santos-Torres, Andr\'{e}s and de Torres Coll, Patricia and Bukits, Tam\'{a}s and Peris\'{e}, Ram\'{o}n and Cabrero Barros, Sergio},
title = {The XR Table: Envisioning The Future of Remote Dining Experiences Using Immersive Telepresence},
year = {2025},
isbn = {9798400714856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715336.3735730},
doi = {10.1145/3715336.3735730},
abstract = {Eating is a vital, social, and intimate experience. Eating with others positively influences our health and mood and enhances our social relationships. However, sometimes, we lack the time, or the circumstances do not allow us to have this intimate experience physically. Current technology, such as video calls, somehow helps bridge the gap of physicality. However, they provide only a fraction of the social experience that face-to-face interaction allows. Recent progress in Immersive Telepresence and eXtended Reality (XR) enables an opportunity to redefine digital commensality, not only to overcome the gap of physical distance, but also to go beyond and redefine the experience of eating. In this paper, we–a multidisciplinary team of technology and commensality researchers–envision the future of digital commensality through immersive technology. To this end, we conducted two participatory co-design workshops and used iterative analysis to identify a set of design criteria that were applied to a working prototype system: the XR Table.},
booktitle = {Proceedings of the 2025 ACM Designing Interactive Systems Conference},
pages = {1674–1690},
numpages = {17},
keywords = {Participatory Design, Digital Commensality, Volumetric Video, Social XR},
location = {
},
series = {DIS '25}
}

@inproceedings{10.1145/3715669.3725882,
author = {Lopez-Cardona, Angela and Emami, Parvin and Idesis, Sebastian and Duraisamy, Saravanakumar and Leiva, Luis A. and Arapakis, Ioannis},
title = {A Comparative Study of Scanpath Models in Graph-Based Visualization},
year = {2025},
isbn = {9798400714870},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715669.3725882},
doi = {10.1145/3715669.3725882},
abstract = {Information Visualization (InfoVis) systems utilize visual representations to enhance data interpretation. Understanding how visual attention is allocated is essential for optimizing interface design. However, collecting Eye-tracking (ET) data presents challenges related to cost, privacy, and scalability. Computational models provide alternatives for predicting gaze patterns, thereby advancing InfoVis research. In our study, we conducted an ET experiment with 40 participants who analyzed graphs while responding to questions of varying complexity within the context of digital forensics. We compared human scanpaths with synthetic ones generated by models such as DeepGaze, UMSS, and Gazeformer. Our research evaluates the accuracy of these models and examines how question complexity and number of nodes influence performance. This work contributes to the development of predictive modeling in visual analytics, offering insights that can enhance the design and effectiveness of InfoVis systems.},
booktitle = {Proceedings of the 2025 Symposium on Eye Tracking Research and Applications},
articleno = {89},
numpages = {11},
keywords = {Eye-tracking, Computer Vision, Deep Learning, Scanpaths},
location = {
},
series = {ETRA '25}
}

@inproceedings{10.1145/3698061.3726940,
author = {Serra Navarro, David},
title = {The Myth of the Cave. Generated shadows and co-creation of light},
year = {2025},
isbn = {9798400712890},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3698061.3726940},
doi = {10.1145/3698061.3726940},
abstract = {The Myth of the Cave is an artwork generated by artificial intelligence (AI), a co-creation that reinterprets Plato's famous allegory to invite us to reflect on how we perceive and understand reality. Using photographs recovered from historical archives, images that documented everyday reality (1900-1930), AI is integrated into the creative process to offer a speculative prediction of the moments captured in silver gelatin. Apparently, they are fictitious recreations generated by AI, however, the piece visually explores, from the generated story, the intersubjective mechanisms in which we build our historical memory. It is a dialogue with the past to develop a story of the future recreated by an algorithm that has been fed by collective imaginaries. A technology that allows us to observe not only the shadows projected in the cave, but also the fire that generates them, the objects that create these shadows and the forces that control the experience.},
booktitle = {Proceedings of the 2025 Conference on Creativity and Cognition},
pages = {842–845},
numpages = {4},
keywords = {Art-based research, Design Fiction, Hybrid Story, human-AI Co-creation},
location = {
},
series = {C&amp;C '25}
}

@inproceedings{10.1145/3689050.3705998,
author = {Bertran, Ferran Altarriba and M\'{a}rquez Puig, Jordi and Corominas, Jur},
title = {An In-Progress Research through Design Exploration of the Frictions Emerging in Playful Public Expression Systems (and How to Navigate Them)},
year = {2025},
isbn = {9798400711978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689050.3705998},
doi = {10.1145/3689050.3705998},
abstract = {In this paper, we present our in-progress Research through Design around PixelGrid, an interactive display for producing pixel art and showcasing it publicly. What started as an exploration into the design of interactive tech that affords playful expression in public spaces progressively evolved into an inquiry into how to handle the unwanted uses these systems may afford, i.e. the possibility that people use their affordances in ways that might be susceptible of being offensive to others. Here we present the current state of our research: (1) describing the latest version of our prototype; (2) reporting on the iterative process that led to it, as well as on its latest deployment; and (3) sharing the early insights that derived from our in-progress research. We will present both those insights and our prototype at TEI'25, to discuss our in-progress learnings and collectively envision future avenues for our research.},
booktitle = {Proceedings of the Nineteenth International Conference on Tangible, Embedded, and Embodied Interaction},
articleno = {59},
numpages = {7},
keywords = {Playable cities, Playful technology, Playfulness, Public expression, Public space},
location = {
},
series = {TEI '25}
}

@inproceedings{10.1145/3706598.3713540,
author = {Ma, Rongjun and Maidhof, Caterina and Carrillo, Juan Carlos and Lindqvist, Janne and Such, Jose},
title = {Privacy Perceptions of Custom GPTs by Users and Creators},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713540},
doi = {10.1145/3706598.3713540},
abstract = {GPTs are customized LLM apps built on OpenAI’s large language model. Any individual or organization can use and create GPTs without needing programming skills. However, the rapid proliferation of over three million GPTs has raised significant privacy concerns. To explore the privacy perspectives of users and creators, we interviewed 23 GPT users with varying levels of creation experience. Our findings reveal blurred lines between user and creator roles and their understanding of GPT data flows. Participants raised concerns about data handling during collection, processing, and dissemination, alongside the lack of privacy regulations. Creators also worried about loss of their proprietary knowledge. In response, participants adopted practices like self-censoring input, evaluating GPT actions, and minimizing usage traces. Focusing on the dual role of user-creators, we find that expertise and responsibility shape privacy perceptions. Based on these insights, we propose practical recommendations to improve data transparency and platform regulations.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {237},
numpages = {18},
keywords = {GPTs, LLM Apps, Privacy Concerns, Privacy Practices, Interviews, Empirical Studies},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3699682.3727565,
author = {Garcia, Victor},
title = {Towards Personalized Physiotherapy via Common Semantic Fusion: Multi-Modal Learning, Computer Vision and Empathetic NLP},
year = {2025},
isbn = {9798400713132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3699682.3727565},
doi = {10.1145/3699682.3727565},
abstract = {This research develops an AI-driven framework for personalized physiotherapy by integrating multi-modal learning, computer vision, and empathetic NLP. It focuses on user modeling and personalization to enhance physiotherapy assessments via an optimized YOLO Pose algorithm, fusing visual, auditory, and textual data for comprehensive mobility evaluation. Preliminary results show improved pose estimation, supporting the potential for clinical validation and integration of additional modalities such as inertial sensors.},
booktitle = {Proceedings of the 33rd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {424–428},
numpages = {5},
keywords = {Physiotherapy, YOLO Pose, Multi-Modal Learning, Personalized Physiotherapy, Semantic Framework, Empathetic NLP, Deep Learning},
location = {
},
series = {UMAP '25}
}

@inproceedings{10.1145/3699682.3727566,
author = {Rapado, Dayris},
title = {Cognitive-Emotional Modeling and Hybrid Intelligence: A User-Centered Approach to Psychomotor Interventions in Active Aging},
year = {2025},
isbn = {9798400713132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3699682.3727566},
doi = {10.1145/3699682.3727566},
abstract = {Aging presents challenges to society. Scientific research suggests that regular physical activity and cognitive stimulation are essential for an active aging process. In this context, this research proposes a novel approach that combines hybrid intelligence and neuroscience to model the cognitive functions and emotional state of people to generate psychomotor interventions to promote active aging.},
booktitle = {Proceedings of the 33rd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {379–382},
numpages = {4},
keywords = {Active aging, Cognitive-emotional modeling, User modeling, Hybrid intelligence, Explainability, AI human-centric, Biomarkers},
location = {
},
series = {UMAP '25}
}

@inproceedings{10.1145/3737821.3749558,
author = {Roig-Maim\'{o}, Maria Francesca and Mas-Sans\'{o}, Ramon and MacKenzie, I. Scott},
title = {Tracing: The Forgotten Task of ISO 9241},
year = {2025},
isbn = {9798400719707},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3737821.3749558},
doi = {10.1145/3737821.3749558},
abstract = {The tests in ISO&nbsp;9241 to evaluate pointing, selecting, and dragging tasks are well-known and widely used. However, although tracing is crucial in many mobile applications, like drawing or artwork, the tracing tests (e.g., path-following, line tracing) are mostly overlooked. In this paper, we detail the calculation of throughput for tracing tasks, noting ambiguities and inconsistencies in the definitions and equations in the ISO standard. A user study with 16 participants was conducted to explore the equations for the effective index of difficulty (({it ID}_text{e})) for the two ISO tests for tracing. Results show the importance of the logarithmic term to preserve the theoretical foundation of Fitts’ law and comparability between studies. We also recommend the use of throughput, calculated as describe herein, as a performance metric when using the tracing tests.},
booktitle = {Adjunct Proceedings of the 27th International Conference on Mobile Human-Computer Interaction},
articleno = {15},
numpages = {7},
keywords = {ISO 9241-9, ISO 9241-411, tracing, Fitts’ law, FittsTracing},
location = {
},
series = {MobileHCI '25 Adjunct}
}

@inproceedings{10.1145/3715336.3735777,
author = {Turmo Vidal, Laia and Tajadura-Jim\'{e}nez, Ana and Ley-Flores, Judith},
title = {Temporal Trajectories: Characterizing Somatic Experiences that Unfold Over Time},
year = {2025},
isbn = {9798400714856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715336.3735777},
doi = {10.1145/3715336.3735777},
abstract = {The body technologies we design profoundly influence our somatic experiences, yet they are often evaluated through short-term or one-off studies. To design for sustained, longer-term engagements, we need to understand how somatic experiences evolve when people repeatedly interact with the same technology over time. With this goal, we report on two in-the-wild studies of body sonification, one with physically inactive individuals and another with professional dancers. For one month, participants used SoniBand, a movement sonification wearable, in their daily lives and shared their experiences with us through questionnaires and in-depth interviews. Drawing from the concept of trajectories, we identified four temporal patterns that characterized the participants’ evolving experience with SoniBand: singular, sustained, deepening, and meandering. We unpack these temporal trajectories and reflect on the characteristics that may contribute to their emergence. Our findings offer insights for studying and designing future technologies that embrace the dynamic, evolving nature of people’s somatic experiences.},
booktitle = {Proceedings of the 2025 ACM Designing Interactive Systems Conference},
pages = {2931–2949},
numpages = {19},
keywords = {Movement sonification, wearable, temporality, sensory feedback, somatic experiences, soma, embodied interaction, dance},
location = {
},
series = {DIS '25}
}

@inproceedings{10.1145/3706599.3706640,
author = {MacKenzie, Scott and Roig-Maim\'{o}, Maria Francesca and Mas-Sans\'{o}, Ramon},
title = {Empirical Research Methods for Human-Computer Interaction},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3706640},
doi = {10.1145/3706599.3706640},
abstract = {Most attendees at CHI conferences will agree that an experiment (user study) is the hallmark of good research in human-computer interaction. But what constitutes an experiment? And how does one go from an experiment to a CHI paper?This course will teach how to pose testable research questions, how to make and measure observations, and how to design and conduct an HCI experiment. Specifically, attendees will participate in a real experiment to gain experience as both an investigator and as a participant. The second session covers the statistical tools typically used to analyze data. Most notably, attendees will learn how to organize experiment results and write a CHI paper.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {812},
numpages = {3},
keywords = {Empirical research; user study; experiment design; quantitative methods; writing a CHI paper.},
location = {
},
series = {CHI EA '25}
}

@article{10.1145/3711033,
author = {Chen, Qijia and Bellucci, Andrea and Jacucci, Giulio},
title = {Mirror Dwellers in Social VR: Investigating Reasons and Perception of Mirror Watching},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
url = {https://doi.org/10.1145/3711033},
doi = {10.1145/3711033},
abstract = {In social Virtual Reality (VR) environments, the significant trend of 'mirror dwellers,' users who often use virtual mirrors to engage with their avatars, has emerged. This study examines discussions from r/VRchat to explore the reasons for this behavior and how it is perceived within the broader community. Our findings highlight the critical role of mirrors in compensating for the sensory limitations of VR, particularly the lack of physical feedback. Users often turn to mirrors to view parts of their avatar that are not accessible from a first-person perspective. Additionally, our research uncovers that a limited Field-Of-View (FOV) hinders the development of a strong connection between users and their avatars, further driving the need for mirrors. However, while using mirrors to mitigate FOV and physical feedback limitations can be helpful, it may also disrupt social interaction in VR environments, as the excessive reliance on mirrors can hinder the social experience in VR for others. This research deepens our understanding of user behavior in social VR and provides insights that could guide future design improvements to enrich the overall user experience.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = may,
articleno = {CSCW135},
numpages = {23},
keywords = {mirror, social VR, virtual reality}
}

@article{10.1145/3711011,
author = {Delaunay, Julien and Gal\'{a}rraga, Luis and Largouet, Christine and van Berkel, Niels},
title = {Impact of Explanation Techniques and Representations on Users' Comprehension and Confidence in Explainable AI},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
url = {https://doi.org/10.1145/3711011},
doi = {10.1145/3711011},
abstract = {Local explainability, an important sub-field of eXplainable AI, focuses on describing the decisions of AI models for individual use cases by providing the underlying relationships between a model's inputs and outputs. While the machine learning community has made substantial progress in improving explanation accuracy and completeness, these explanations are rarely evaluated by the final users. In this paper, we evaluate the impact of various explanation and representation techniques on users' comprehension and confidence. Through a user study on two different domains, we assessed three commonly used local explanation techniques—feature-attribution, rule-based, and counterfactual—and explored how their visual representation—graphical or text-based—influences users' comprehension and trust. Our results show that the choice of explanation technique primarily affects user comprehension, whereas the graphical representation impacts user confidence.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = may,
articleno = {CSCW113},
numpages = {28},
keywords = {explainability, interpretability, machine learning, user studies}
}

@inproceedings{10.1145/3706598.3713368,
author = {Troiano, Giovanni M and Cassidy, Michael and Morales, Daniel Escobar and Pons, Guillermo and Abdollahi, Amir and Robles, Gregorio and Puttick, Gillian and Harteveld, Casper},
title = {CT4ALL: Towards Putting Teachers in the Loop to Advance Automated Computational Thinking Metric Assessments in Game-Based Learning},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713368},
doi = {10.1145/3706598.3713368},
abstract = {Computational thinking (CT) is essential for the 21st century learner. Yet, assessing CT remains challenging. This is particularly challenging in constructionist learning, where individual idiosyncrasies may clash with one-size-fits-all assessments. Tools like Dr. Scratch offer CT metrics that show promise for effective and scalable CT assessments, particularly in constructionist game-based learning (GBL). Prior work has advanced the design of automated CT metrics but hardly included teachers in the process. We extend Dr. Scratch to improve automated CT assessments for GBL and put teachers in the loop to assess its novel features. Specifically, we interviewed seven middle school teachers employing GBL in STEM curricula and asked them to provide feedback on the newly designed CT metrics. Teachers view the new CT metrics positively, underscoring their potential for adaptive CT assessments despite hindrances. We advance automated CT assessments via teacher evaluation toward design-sensitive CT metrics and CT for all.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {124},
numpages = {23},
keywords = {Computational thinking, automated metrics, computer science education, STEM, assessment},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3706598.3713151,
author = {Altarriba Bertran, Ferran and Buruk, O\u{g}uz 'Oz' and M\'{a}rquez Puig, Jordi and Hamari, Juho},
title = {How Can Interactive Technology Help Us to Experience Joy With(in) the Forest? Towards a Taxonomy of Tech for Joyful Human-Forest Interactions},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713151},
doi = {10.1145/3706598.3713151},
abstract = {This paper presents intermediate-level knowledge in the form of a taxonomy that highlights 12 different ways in which interactive tech might support forest-related experiences that are joyful for humans. It can inspire and provide direction for designs that aim to enrich the experiential texture of forests. The taxonomy stemmed from a reflexive analysis of 104 speculative ideas produced during a year-long co-design process, where we co-experienced and creatively engaged a diverse range forests and forest-related activities with 250+ forest-goers with varied backgrounds and sensitivities. Given that breadth of forests and populations involved, our work foregrounds a rich set of design directions that set an actionable early frame for creating tech that supports joyful human-forest interplays – one that we hope will be extended and consolidated in future research, ours and others'.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {742},
numpages = {21},
keywords = {Nature, celebratory tech, forests, interactive tech, joy, play, speculative design},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3689050.3708393,
author = {Vega-Cebri\'{a}n, Jos\'{e} Manuel and Lindegren, Andreas and Gamboa, Mafalda and Tajadura-Jim\'{e}nez, Ana and Fernaeus, Ylva and M\'{a}rquez Segura, Elena},
title = {Embodied Ideation, Toolkits, and Sketching},
year = {2025},
isbn = {9798400711978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689050.3708393},
doi = {10.1145/3689050.3708393},
abstract = {Movement-based design foregrounds the moving and sentient body, fostering holistic engagement with the surrounding physical, material, and sociospatial contexts. Over the years, this approach has yielded multiple methodologies, tools, and exemplars to support body-based ideation. In this studio, we explore tools that facilitate embodied thinking and the creative processes of designing with, through, and for the body. In particular, we focus on: i) embodied ideation tools, kits, and technology probes to prompt ideation; ii) sketching and other documentation techniques to materialize ephemeral embodied action during ideation. We will bring tools and techniques to engage with both aspects, and we will invite participants to bring their own, which can be physical or technological, low or high-fidelity. This hands-on studio will provide a space to collectively engage in embodied ideation and sketching; exploring, analysing, and engaging deeply with the available objects and methods. The studio will culminate in a rich set of visual material and an annotated portfolio, which will be shared with the broader community, fostering connections among designers interested in movement-based and tangible design.},
booktitle = {Proceedings of the Nineteenth International Conference on Tangible, Embedded, and Embodied Interaction},
articleno = {128},
numpages = {6},
keywords = {Movement-based Design, Body-based Design, Soma Design, Toolkit, Technology Probes, Embodied Sketching, Body Perception, Body Movement, Multisensory Feedback, Biofeedback, Ideation, Bodystorming, Bodystorming Basket, Ideation Props, Ideation Probes, Sketching},
location = {
},
series = {TEI '25}
}

@inproceedings{10.1145/3706598.3713273,
author = {Shin, Joongi and Polyanskaya, Anna and Lucero, Andr\'{e}s and Oulasvirta, Antti},
title = {No Evidence for LLMs Being Useful in Problem Reframing},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713273},
doi = {10.1145/3706598.3713273},
abstract = {Problem reframing is a designerly activity wherein alternative perspectives are created to recast what a stated design problem is about. Generating alternative problem frames is challenging because it requires devising novel and useful perspectives that fit the given problem context. Large language models (LLMs) could assist this activity via their generative capability. However, it is not clear whether they can help designers produce high-quality frames. Therefore, we asked if there are benefits to working with LLMs. To this end, we compared three ways of using LLMs (N = 280): 1) free-form, 2) direct generation, and 3) a structured approach informed by a theory of reframing. We found that using LLMs does not help improve the quality of problem frames. In fact, it increases the competence gap between experienced and inexperienced designers. Also, inexperienced ones perceived lower agency when working with LLMs. We conclude that there is no benefit to using LLMs in problem reframing and discuss possible factors for this lack of effect.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {243},
numpages = {25},
keywords = {Problem-solving, problem reframing, LLM},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3706599.3720274,
author = {Silva, Isabella Barbosa and Oliveira, Elsa and Melo, Ricardo and Rosado, Lu\'{\i}s and G\'{a}lvez-Barr\'{o}n, C\'{e}sar and Heijink, Irene Bernadet and Hoogteijling, Sem and Gabilondo, I\~{n}igo},
title = {Designing for Qualitative Evaluation of Synthetic Medical Data},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3720274},
doi = {10.1145/3706599.3720274},
abstract = {Machine learning in healthcare often struggles with data access for model training due to privacy restrictions, rare conditions, and high acquisition costs. Synthetic data offers a potential workaround, yet there are no agreed-upon gold standards for evaluating it. As quantitative metrics alone cannot fully assess the desired qualities of generative model outputs, human inspection is a key component of validation, warranting a “Doctor-in-the-loop” approach. However, research is scarce on best practices for interaction and user interface design in such systems. This paper presents preliminary designs for qualitative synthetic medical data evaluation, informed by four participatory workshops with seven doctors and nine machine learning engineers. Spanning tabular, image, and time series data, this study emphasised transparency and clear communication of the synthetic data generation. In addition to presenting the rationale behind the evaluation workflow design, we highlight challenges in the medical domain, including doctors’ limited familiarity and skepticism with synthetic data.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {185},
numpages = {11},
keywords = {Synthetic Data (SD), Synthetic medical data (SMD), Doctor-in-the-Loop (DITL), Qualitative Evaluation, Participatory Design, Machine Learning in Healthcare, Human-Computer Interaction (HCI)},
location = {
},
series = {CHI EA '25}
}

@article{10.1145/3710965,
author = {Purohit, Hemant and Buntain, Cody and Hughes, Amanda Lee and Peterson, Steve and Lorini, Valerio and Castillo, Carlos},
title = {Engage and Mobilize! Understanding Evolving Patterns of Social Media Usage in Emergency Management},
year = {2025},
issue_date = {May 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
url = {https://doi.org/10.1145/3710965},
doi = {10.1145/3710965},
abstract = {The work of Emergency Management (EM) agencies requires timely collection of relevant data to inform decision-making for operations and public communication before, during, and after a disaster. However, the limited human resources available to deploy for field data collection is a persistent problem for EM agencies. Thus, over the last decade, many of these agencies have started leveraging social media as a supplemental data source and a new venue to engage with the public. Such uses present both opportunities and challenges. While prior research has analyzed the potential benefits and attitudes of practitioners and the public when leveraging social media during disasters, a gap exists in the critical analysis of the actual practices and uses of social media among EM agencies, across both geographical regions and phases of the EM lifecycle - typically mitigation, preparedness, response, and recovery. In this paper, we conduct a mixed-method analysis to update and fill this gap on how EM practitioners in the U.S. and Europe use social media, building on a survey study of about 150 professionals and a follow-up interview study with 11 participants. The results indicate that using social media is no longer a non-traditional practice in operational and informational processes for the decision-making of EM agencies working at both the local level (e.g., county or town) and non-local level (e.g., state/province, federal/national) for emergency management. Especially, the practitioners affiliated with agencies working at the local level have a very high perceived value of social media for situational awareness (e.g., analyzing disaster extent and impact) and public communication (e.g., disseminating timely information and correcting errors in crisis coverage). Further, practitioners now engage with the public during the preparedness phase to mobilize them during the response phase. We present a model to understand the current practices of communication between agencies and the public, as well as among practitioners while leveraging social media. We also discuss novel challenges, including public fragmentation caused by the increasing use of multiple social media platforms, information integrity, and social listening expectations. We conclude with the policy, technological, and socio-technical needs to design future social media analytics systems to support the work of EM agencies in such communication.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = may,
articleno = {CSCW067},
numpages = {39},
keywords = {crises, crisis informatics, disasters, practitioners, social media}
}

@inproceedings{10.1145/3706598.3713959,
author = {Hassan, Waseem and Da, Liyue and Elizondo, Sonia and Hornb\ae{}k, Kasper},
title = {Heartbeat Resonance: Inducing Non-contact Heartbeat Sensations in the Chest},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713959},
doi = {10.1145/3706598.3713959},
abstract = {Perceiving and altering the sensation of internal physiological states, such as heartbeats, is key for biofeedback and interoception. Yet, wearable devices used for this purpose can feel intrusive and typically fail to deliver stimuli aligned with the heart’s location in the chest. To address this, we introduce Heartbeat Resonance, which uses low-frequency sound waves to create non-contact haptic sensations in the chest cavity, mimicking heartbeats. We conduct two experiments to evaluate the system’s effectiveness. The first experiment shows that the system created realistic heartbeat sensations in the chest, with 78.05 Hz being the most effective frequency. In the second experiment, we evaluate the effects of entrainment by simulating faster and slower heart rates. Participants perceived the intended changes and reported high confidence in their perceptions for +15\% and -30\% heart rates. This system offers a non-intrusive solution for biofeedback while creating new possibilities for immersive VR environments.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {913},
numpages = {22},
keywords = {Vibrotactile feedback; non-contact haptics; psychophysics; heartbeat modulation},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3699682.3728345,
author = {Waterschoot, Cedric and Yera Toledo, Raciel and Tintarev, Nava and Barile, Francesco},
title = {With Friends Like These, Who Needs Explanations? Evaluating User Understanding of Group Recommendations},
year = {2025},
isbn = {9798400713132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3699682.3728345},
doi = {10.1145/3699682.3728345},
abstract = {Group Recommender Systems (GRS) employing social choice-based aggregation strategies have previously been explored in terms of perceived consensus, fairness, and satisfaction. At the same time, the impact of textual explanations has been examined, but the results suggest a low effectiveness of these explanations. However, user understanding remains fairly unexplored, even if it can contribute positively to transparent GRS. This is particularly interesting to study in more complex or potentially unfair scenarios when user preferences diverge, such as in a minority scenario (where group members have similar preferences, except for a single member in a minority position). In this paper, we analyzed the impact of different types of explanations on user understanding of group recommendations. We present a randomized controlled trial (n = 271) using two between-subject factors: (i) the aggregation strategy (additive, least misery, and approval voting), and (ii) the modality of explanation (no explanation, textual explanation, or multimodal explanation). We measured both subjective (self-perceived by the user) and objective understanding (performance on model simulation, counterfactuals and error detection). In line with recent findings on explanations for machine learning models, our results indicate that more detailed explanations, whether textual or multimodal, did not increase subjective or objective understanding. However, we did find a significant effect of aggregation strategies on both subjective and objective understanding. These results imply that when constructing GRS, practitioners need to consider that the choice of aggregation strategy can influence the understanding of users. Post-hoc analysis also suggests that there is value in analyzing performance on different tasks, rather than through a single aggregated metric of understanding.},
booktitle = {Proceedings of the 33rd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {253–262},
numpages = {10},
keywords = {Group Recommender Systems, Social Choice-Based Explanations, Objective Understanding, Subjective Understanding, User Study},
location = {
},
series = {UMAP '25}
}

@inproceedings{10.1145/3708319.3727561,
author = {Purificato, Erasmo and Boratto, Ludovico and Vinagre, Jo\~{a}o},
title = {Data Access under the EU Digital Services Act and its Impact on User Modelling Research},
year = {2025},
isbn = {9798400713996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3708319.3727561},
doi = {10.1145/3708319.3727561},
abstract = {The Digital Services Act (DSA) establishes a regulatory framework for online platforms and search engines in the European Union, focusing on mitigating systemic risks such as illegal content dissemination, fundamental rights violations, and impacts on electoral processes, public health, and gender-based violence. Very Large Online Platforms (VLOPs) and Very Large Search Engines (VLOSEs), defined as those with over 45 million active recipients, must provide data access for research to enable investigations into these risks and the development of solutions. This tutorial is tailored for the UMAP community, addressing the implications of the DSA for user modelling research. It will cover the DSA’s key provisions and definitions, outline the procedural steps for accessing VLOP and VLOSE data, and discuss the technical aspects of data access requests. Participants will also explore the challenges and opportunities involved in working with this data. By the end of the tutorial, attendees will have a thorough understanding of the DSA’s data access provisions, the technical and procedural requirements for accessing VLOP and VLOSE data, and the regulation’s implications for user modelling research. They will be equipped to navigate the complexities of the DSA and contribute to the development of responsible and transparent online platforms.Further information and resources about the tutorial are available on the website: https://erasmopurif.com/tutorial-dsa-umap25/.},
booktitle = {Adjunct Proceedings of the 33rd ACM Conference on User Modeling, Adaptation and Personalization},
pages = {5–7},
numpages = {3},
keywords = {Digital Services Act, Online Platform Regulation, Data Access, User Modelling Research, Responsible AI},
location = {
},
series = {UMAP Adjunct '25}
}

@inproceedings{10.1145/3715336.3735722,
author = {Tumedei, Gianni and Ceccarini, Chiara and Jimenez Navarro, Inmaculada Concepci\'{o}n and Prandi, Catia},
title = {From Drawings to Awareness: Exploring Narrative Visualization and AI to Teach Children About the Fragile Ecosystem of the Mar Menor Lagoon},
year = {2025},
isbn = {9798400714856},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715336.3735722},
doi = {10.1145/3715336.3735722},
abstract = {Marine ecosystems are vital for human survival and the well-being of our planet. Educating children about marine conservation and environmental preservation is essential for fostering future generations that value and adopt sustainable practices, particularly in fragile ecosystems like the Mar Menor, a coastal saltwater lagoon in Spain severely affected by climatic and anthropogenic pressures. To enhance engagement in learning about the lagoon’s ecosystem and challenges, we developed an interactive system that integrates children’s drawings into a narrative visualization, combined with a conversational interface powered by generative AI. We conducted an evaluation with 52 children aged 10-12, where the system was framed in an environmental education activity. Results demonstrated a 33.3\% increase in children’s understanding of the lagoon’s critical condition and a 23.1\% improvement in their awareness of key contributing factors, particularly climate change. Additionally, the activity fostered curiosity about the lagoon, concern for its situation, and attitude toward conservation efforts.},
booktitle = {Proceedings of the 2025 ACM Designing Interactive Systems Conference},
pages = {2684–2700},
numpages = {17},
keywords = {Interactive Narrative Visualization, Conversational User Interface, Generative AI, Children-Made Drawings, Environmental Education},
location = {
},
series = {DIS '25}
}

@inproceedings{10.1145/3706599.3719695,
author = {Cumbal, Ronald and Calvo-Barajas, Natalia and Escobar-Planas, Marina and Rouchitsas, Alexandros and Castellano, Ginevra},
title = {Visualizing Confidence in Delivery Robots: Insights from Two Online Studies},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3719695},
doi = {10.1145/3706599.3719695},
abstract = {The growing deployment of mobile robots for last-mile delivery requires their safe and efficient navigation in urban environments. Although considerable research has focused on enhancing robots’ understanding of human movement, optimizing communication strategies remains an underexplored area. Transparent communication of a robot’s intentions can help reduce confusion and facilitate safe interactions with nearby pedestrians. This study investigates how visual displays of a robot’s confidence in its navigation intentions can enhance human-robot interaction. An initial online study demonstrates that both numerical displays and bar graphs improved interpretability. A second online study finds an effect of confidence communication on participants’ perception of the robot’s predictability, although this effect did not extend to other measures. These results align with previous research on communicating confidence in human-robot interaction, yet discrepancies arose regarding the methods of communication. Our findings contribute to the development of communication strategies that may enhance interactions between robots and pedestrians.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {578},
numpages = {7},
keywords = {Human-robot interaction, Uncertainty, eHMI, Design, Transparency, Explainability, Trust, Predictability, Safety},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3706599.3719961,
author = {He, Shijing and Ma, Chenkai and Zhang, Chi and Abu-Salma, Ruba and Such, Jose},
title = {"Living-Alone Girls' Lives Matter": Exploring the Security and Safety Perceptions and Practices of Young Women Living Alone in China},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3719961},
doi = {10.1145/3706599.3719961},
abstract = {Increasing numbers of young women living alone face a wide range of security and safety (SS) concerns such as harassment, surveillance, and violence in today’s China. Through thematic analysis and feminist critical discourse analysis (FCDA) of SS-related posts on RedNote (n = 404), a popular social platform in China, we uncovered the SS practices of living-alone women (e.g., deploying smart home devices, improving masculinity), and how social media amplifies fears, ignites feminist debates, and exacerbates gender inequality in China. Within China’s unique socio-cultural environment—characterized by traditional gender norms, government surveillance, and the rapid adoption of surveillance technologies—we found that women’s SS concerns are commodified, shifting responsibility onto individuals rather than driving systemic reforms. As a result, patriarchal power structures remain intact, while state and legal systems fail to address the root causes of violence.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {23},
numpages = {8},
keywords = {Security, safety, feminism, gender inequality, discourse analysis, social media},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3706598.3713823,
author = {Turmo Vidal, Laia and Waern, Annika and Cabanas-Vald\'{e}s, Rosa and van Loo, Lauren and Li, Yinchu and Meenaakshisundaram, Karthik Venkataraman},
title = {Towards Personalized Physiotherapy through Interactive Machine Learning: A Conceptual Infrastructure Design for In-Clinic and Out-of-Clinic Support},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713823},
doi = {10.1145/3706598.3713823},
abstract = {Machine learning (ML) is increasingly used in healthcare practices, due to its potential to support personalization, diagnostic and prediction, automatization, and increase effectiveness. In physiotherapy, most existing ML solutions suggest replacing the physiotherapist, neglecting the complexity of their skills and practice. We articulate an alternative to the design of ML technology for physiotherapy: one that emphasizes the relational aspects of the practice and offers personalized support to physiotherapists and patients alike. Based on domain studies and design explorations with physiotherapists, interaction designers and ML experts, we present 1) insights on physiotherapy’s in-clinic and out-of-clinic looped structure, 2) opportunities and requirements to integrate ML in that loop, and 3) a conceptual interactive ML-based infrastructure that exploits those opportunities. Our work widens current ML developmental aims for physiotherapy, proposing a vision that encodes sustainable sociotechnical relationships in healthcare practices.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {670},
numpages = {19},
keywords = {Machine Learning, Physiotherapy, Personalization, Training, Interactive Machine Learning, Conceptual Design},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3706598.3714244,
author = {Chen, Qijia and Bellucci, Andrea and Cai, Jie and Nelimarkka, Matti and Jacucci, Giulio},
title = {Understanding "Mutes" in Social Virtual Reality},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714244},
doi = {10.1145/3706598.3714244},
abstract = {In social Virtual Reality (VR), particularly within VRChat, a significant group of users often referred to as “mutes” refrain from voice communication. This study analyzes 4212 discussion entries, including both original submissions and comments, from the r/VRchat subreddit to explore the experiences and reasons behind this practice. Our findings indicate that muteness is an integral aspect of social VR culture, yet mute users face challenges, including exposure to abusive behaviors and communication barriers in a fast-paced environment. Factors of social VR like harassment, heightened social anxiety from the immersive presence, and the complexities of identity management can discourage voice communication, leading many to adopt “muteness” as a response. This behavior can be seen within the broader context of social disability, challenging normative communication assumptions. We highlight the risks of generalizing marginalized communities and emphasize the need for further research to address and support the unique needs of these groups in social VR spaces.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {40},
numpages = {17},
keywords = {virtual reality, social VR, online harassment, mute, disability},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3706599.3719855,
author = {Boudouraki, Andriana and Cameron, Harriet R and Reyes-Cruz, Gisela and Martinez Avila, Juan Pablo and Orduna, Marta and Varela Castro, Samanta and Kohonen-Aho, Laura},
title = {Contesting Control with Automation in Technology-Mediated Interactions},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3719855},
doi = {10.1145/3706599.3719855},
abstract = {With the growing use of autonomous systems in computer-supported communication(CSC) such as recommended text in emails, speaker tracking in videoconferencing or automated facial expressions in VR avatars, machine agency is increasingly becoming entwined with human agency in how we enact our identity through media. It is important to critically examine how this affects the way we communicate and how users engage with surrendering or maintaining control of their self-expression. Using the example of Mobile Robotic Telepresence, we demonstrate the use of the Contesting Control framework as a lens for examining interview and video data, to understand how control over automation is conducted in technology-mediated, social interactions. Reflecting on our ongoing work, we propose some additions to the framework and urge the research community to further examine the implications of automating mediated communication. In doing so we hope to inspire the design and development of autonomous technologies and features in this field.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {150},
numpages = {7},
keywords = {telepresence, videoconferencing, autonomous systems, identity},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3737821.3743747,
author = {Gruenerbl, Agnes and Spilski, Jan and Barbareschi, Giulia and Kunze, Kai and ElAgroudy, Passant and Lachmann, Thomas and Lukowicz, Paul},
title = {2nd International Workshop on Mobile Cognitive-Augmenting and Cognition-Altering Technologies (CAT) in Human-Centered AI},
year = {2025},
isbn = {9798400719707},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3737821.3743747},
doi = {10.1145/3737821.3743747},
abstract = {The quest for enhanced cognition is still a driving force behind human advancement. Following the success of the first installment of the mobiCHAI workshop in 2024, mobiCHAI2 will continue to explore the intersection of Mobile Cognition-Altering Technologies (CAT) and Human-Centered AI (HCAI), focusing on their potential to augment and modify human cognition in real-world applications. Building upon previous discussions, this workshop aims to bridge insights from cognitive science, AI, and human-computer interaction (HCI) to address key challenges and opportunities in developing trustworthy, effective, and ethical AI-driven cognitive augmentation tools. Core themes of the workshop include ubiquitous sensing for cognitive tracking, AI-driven cognitive modeling, interactive augmentation methods, and ethical implications of deploying CAT in education, healthcare, and productivity. Special attention is given to the societal and ethical impact of cognition-altering AI, ensuring that augmentation technologies enhance human agency rather than compromise autonomy. Through interdisciplinary collaboration, the workshop fosters discussions on how AI can complement, rather than replace, human cognitive abilities, setting a foundation for responsible and socially acceptable digitization.},
booktitle = {Adjunct Proceedings of the 27th International Conference on Mobile Human-Computer Interaction},
articleno = {28},
numpages = {5},
keywords = {Human-Centered AI; Hybrid-Human Artificial Intelligence; cognitive science; augmenting human capabilities; ubiquitous technologies; shaping cognitive and social behavior; generative AI},
location = {
},
series = {MobileHCI '25 Adjunct}
}

@inproceedings{10.1145/3706598.3713286,
author = {Feick, Martin and Tang, Xuxin and Garcia-Martin, Raul and Luchianov, Alexandru and Huang, Roderick Wei Xiao and Xiao, Chang and Siu, Alexa and Dogan, Mustafa Doga},
title = {Imprinto: Enhancing Infrared Inkjet Watermarking for Human and Machine Perception},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713286},
doi = {10.1145/3706598.3713286},
abstract = {Hybrid paper interfaces leverage augmented reality to combine the desired tangibility of paper documents with the affordances of interactive digital media. Typically, virtual content can be embedded through direct links (e.g., QR codes); however, this impacts the aesthetics of the paper print and limits the available visual content space. To address this problem, we present Imprinto, an infrared inkjet watermarking technique that allows for invisible content embeddings only by using off-the-shelf IR inks and a camera. Imprinto&nbsp; was established through a psychophysical experiment, studying how much IR ink can be used while remaining invisible to users regardless of background color. We demonstrate that we can detect invisible IR content through our machine learning pipeline, and we developed an authoring tool that optimizes the amount of IR ink on the color regions of an input document for machine and human detectability. Finally, we demonstrate several applications, including augmenting paper documents and objects.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {447},
numpages = {18},
keywords = {augmented reality; mixed reality; infrared imaging; watermarking; digital fabrication},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3706599.3706735,
author = {Strohmeier, Paul and Turmo Vidal, Laia and Vega, Gabriela and Reed, Courtney N. and Mazursky, Alex and AliAbbasi, Easa and Tajadura-Jim\'{e}nez, Ana and Steimle, J\"{u}rgen},
title = {Sensorimotor Devices: Coupling Sensing and Actuation to Augment Bodily Experience},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3706735},
doi = {10.1145/3706599.3706735},
abstract = {An emerging space in interface research is wearable devices that closely couple their sensing and actuation abilities. A well-known example is MetaLimbs [39], where sensed movements of the foot are directly mapped to the actuation of supernumerary robotic limbs. These systems are different from wearables focused on sensing, such as fitness trackers, or wearables focused on actuation, such as VR headsets. They are characterized by tight coupling between the user’s action and the resulting digital feedback from the device, in time, space, and mode. The properties of this coupling are critical for the user’s experience, including the user’s sense of agency, body ownership, and experience of the surrounding world. Understanding such systems is an open challenge, which requires knowledge not only of computer science and HCI, but also Psychology, Physiology, Design, Engineering, Cognitive Neuroscience, and Control Theory. This workshop aims to foster discussion between these diverse disciplines and to identify links and synergies in their work, ultimately developing a common understanding of future research directions for systems that intrinsically couple sensing and action.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {799},
numpages = {7},
keywords = {sensorimotor interaction, wearables, feedback systems, motion-coupled feedback},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3689050.3708335,
author = {Teisanu, Silvia and Campo Woytuk, Nadia and Park, Joo Young and Brynskov, Anna and Hua, Dianya Mia and Ciolfi Felice, Marianela and Baeza Arg\"{u}ello, Sa\'{u}l and Tomico, Oscar and Balaam, Madeline},
title = {Designing for and with Intimate (Sexual) Bodies - Towards Feminist and Queer Somatic Understandings of Pleasure},
year = {2025},
isbn = {9798400711978},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689050.3708335},
doi = {10.1145/3689050.3708335},
abstract = {This studio explores methods for prototyping with and for intimate and sexual experiences, focusing on a diversity of bodies and understandings of pleasure. We aim to both explore how to communicate people's somatic (sexual) profiles while engaging in creative making processes. Participants will engage in a full day workshop consisting of three distinct phases: trying out methods for representing somatic experiences, tangible ideation and making, and group reflections. Through these activities, we not only aim to enhance individual awareness of erotic bodies but also share advice and experiences on how to design for sexual experiences from feminist and intersectional perspectives. This studio seeks to promote inclusivity and challenge normative beliefs about sexual bodies, designing for intimacy and contributing to a more equitable discourse on pleasure.},
booktitle = {Proceedings of the Nineteenth International Conference on Tangible, Embedded, and Embodied Interaction},
articleno = {133},
numpages = {6},
keywords = {prototyping for intimate experiences, soma design, workshop},
location = {
},
series = {TEI '25}
}

@inproceedings{10.1145/3719160.3728626,
author = {Munteanu, Cosmin and Sarcar, Sayan and Sin, Jaisie and Ziying Wei, Christina and Sayago, Sergio and Zhao, Wei and Waycott, Jenny and Boostani, Rezvan and Ghafurian, Moojan and Getson, Cristina},
title = {Designing Age-Inclusive Interfaces: Emerging Conversational and Generative AI to Support Interactions across the Life Span},
year = {2025},
isbn = {9798400715273},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3719160.3728626},
doi = {10.1145/3719160.3728626},
abstract = {We are concurrently witnessing two significant shifts: voice and chat-based conversational user interfaces (CUIs) are becoming ubiquitous (especially more recently due to advances in generative AI and LLMs - large language models), and older people are becoming a very large demographic group (and increasingly adopting of mobile technology on which such interfaces are present). However, despite the recent increase in research activity, age-relevant and inter/cross-generational aspects continue to be underrepresented in both research and commercial product design. Therefore, the overarching aim of this workshop is to increase the momentum for research within the space of hands-free, mobile, and conversational interfaces that centers on age-relevant and inter- and cross-generational interaction. For this, we plan to create an interdisciplinary space that brings together researchers, designers, practitioners, and users, to discuss and share challenges, principles, and strategies for designing such interfaces across the life span. We thus welcome contributions of empirical studies, theories, design, and evaluation of hands-free, mobile, and conversational interfaces designed with aging in mind (e.g. older adults or inter/cross-generational). We particularly encourage contributions focused on leveraging recent advances in generative AI or LLMs. Through this, we aim to grow the community of CUI researchers across disciplinary boundaries (human-computer interaction, voice and language technologies, geronto-technologies, information studies, etc.) that are engaged in the shared goal of ensuring that the aging dimension is appropriately incorporated in mobile / conversational interaction design research.},
booktitle = {Proceedings of the 7th ACM Conference on Conversational User Interfaces},
articleno = {37},
numpages = {5},
location = {
},
series = {CUI '25}
}

@inproceedings{10.1145/3715668.3734181,
author = {Ppali, Sophia and Constantinides, Marios and Liarokapis, Fotis and Farao, Jaydon and Anvari, Soraya S. and Yoo, MinYoung and Altarriba Bertran, Ferran and Rodgers, Shannon and Han, Jihae and Wehbe, Rina R. and Brereton, Margot and Covaci, Alexandra},
title = {Cite Your Well-being First: What Happens When Personal Life, Mental Health, and HCI Research Become Entangled?},
year = {2025},
isbn = {9798400714863},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715668.3734181},
doi = {10.1145/3715668.3734181},
abstract = {Human-Computer Interaction (HCI) research often requires deep engagement with people and their environments, making the researcher’s own well-being an integral, yet overlooked factor in the research process. Personal challenges, ranging from academic pressures to difficult life events, can influence how we conduct studies, interpret data, and relate to our work. Despite this, such experiences are rarely acknowledged in formal academic spaces, and there is limited discussion about their impact on research. Our workshop offers a space for HCI researchers to reflect on their well-being, share personal experiences, and examine how personal struggles intersect with their research practices. Together, we will foreground researchers’ well-being as an essential concern and explore how these lived realities can be meaningfully integrated into our methodologies. In doing so, we invite the HCI community to not only centre the human in our research, but also recognise the researcher as human; one whose life is deeply entangled with the work they do.},
booktitle = {Companion Publication of the 2025 ACM Designing Interactive Systems Conference},
pages = {52–56},
numpages = {5},
keywords = {mental health, well-being, personal life, researchers},
location = {
},
series = {DIS '25 Companion}
}

@inproceedings{10.1145/3706599.3716397,
author = {Hourcade, Juan Pablo and Bakala, Ewelina and Bonsignore, Elizabeth and Currin, Flannery Hope and Fails, Jerry Alan and Gilhoi, Amy and Medina Medina, Nuria and Norris, Delaney and Onions, Meredith and Pires, Ana Cristina and Walsh, Greg and Yarosh, Svetlana and Yip, Jason},
title = {Conducting Ethical Research on Emerging Technologies for Children},
year = {2025},
isbn = {9798400713958},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706599.3716397},
doi = {10.1145/3706599.3716397},
abstract = {This SIG will provide child-computer interaction researchers and practitioners, as well as other interested CHI attendees, an opportunity to discuss topics related to conducting ethical research on emerging technologies for children. While the community has extensively debated on ethical issues, we have not had ample discussion of how to ethically manage research on emerging technologies, often designed for adults, that may also be used by children or affect children. More specifically, we would like to discuss ethical aspects related to motivations for research, how research is conducted, and how it is reported. We hope these discussions will go well beyond legal requirements from ethics boards and help provide foundational guidance for research on emerging technologies with children, but also inform similar research with other vulnerable communities.},
booktitle = {Proceedings of the Extended Abstracts of the CHI Conference on Human Factors in Computing Systems},
articleno = {859},
numpages = {3},
keywords = {children, emerging technologies, ethics, research methods},
location = {
},
series = {CHI EA '25}
}

@inproceedings{10.1145/3715669.3726843,
author = {Jakobi, Deborah Noemie and Stegenwallner-Sch\"{u}tz, Maja and Hollenstein, Nora and Ding, Cui and Kaspere, Ramune and Mati\'{c} \v{S}kori\'{c}, Ana and Pavlinusic Vilus, Eva and Frank, Stefan and M\"{u}ller, Marie-Luise and Jensen de L\'{o}pez, Kristine M and Kharlamov, Nik and B. S\o{}ndergaard Knudsen, Hanne and Berzak, Yevgeni and Lion, Ella and Sekerina, Irina A. and Acarturk, Cengiz and Ansari, Mohd Faizan and Harezlak, Katarzyna and Kasprowski, Pawel and Bautista, Ana and Beinborn, Lisa and Bondar, Anna and Boznou, Antonia and Bradshaw, Leah and Hofmann, Jana Mara and Krosness, Thyra and Soliva, Not Battesta and \c{C}epani, Anila and Cergol, Kristina and Do\v{s}en, Ana and Palmovic, Marijan and \c{C}erpja, Adelina and Chirino, Dal\'{\i} and Chrom\'{y}, Jan and Demberg, Vera and \v{S}krjanec, Iza and Deniz, Nazik Din\c{c}topal and Fajardo, Dr. Inmaculada and Gim\'{e}nez-Salvador, Mariola and M\'{\i}nguez-L\'{o}pez, Xavier and Filip, Maro\v{s} and Freibergs, Zigmunds and Gomes, J\'{e}ssica and Janeiro, Andreia and Luegi, Paula and Ver\'{\i}ssimo, Jo\~{a}o and Gramatikov, Sasho and Hasen\"{a}cker, Jana and Haveriku, Alba and Kote, Nelda and Kamal, Muhammad M. and Kundefineddzierska, Hanna and Klimek-Jankowska, Dorota and Kosutar, Sara and Krakowczyk, Daniel G. and Krejtz, Izabela and \L{}ockiewicz, Marta and L\~{o}o, Kaidi and Motiej\={u}nienundefined, Jurgita and Nasir, Jamal A. and Nederg\r{a}rd, Johanne Sofie Krog and \"{O}zkan, Ay\c{s}eg\"{u}l and Preininger, Mikul\'{a}\v{s} and Pung\u{a}, Loredana and Reich, David Robert and Tschirner, Chiara and Rot, \v{S}pela and S\"{a}uberli, Andreas and Sol\'{e}-Casals, Jordi and Strati, Ekaterina and Svoboda, Igor and Trandafili, Evis and Varlokosta, Spyridoula and Vulchanova, Mila and J\"{a}ger, Lena A.},
title = {MultiplEYE: Creating a multilingual eye-tracking-while-reading corpus},
year = {2025},
isbn = {9798400714870},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3715669.3726843},
doi = {10.1145/3715669.3726843},
abstract = {Eye-tracking-while-reading data provide valuable insights across multiple disciplines, including psychology, linguistics, natural language processing, education, and human-computer interaction. Despite its potential, the availability of large, high-quality, multilingual datasets remains limited, hindering both foundational reading research and advancements in applications. The MultiplEYE project addresses this gap by establishing a large-scale, international eye-tracking data collection initiative. It aims to create a multilingual dataset of eye movements recorded during natural reading, balancing linguistic diversity, while ensuring methodological consistency for reliable cross-linguistic comparisons. The dataset spans numerous languages and follows strict procedural, documentation, and data pre-processing standards to enhance eye-tracking data transparency and reproducibility. A novel data-sharing framework, integrated with data quality reports, allows for selective data filtering based on research needs. Researchers and labs worldwide are invited to join the initiative. By establishing and promoting standardized practices and open data sharing, MultiplEYE facilitates interdisciplinary research and advances reading research and gaze-augmented applications.},
booktitle = {Proceedings of the 2025 Symposium on Eye Tracking Research and Applications},
articleno = {111},
numpages = {11},
keywords = {Eye-tracking, reading, psycholinguistics, multilingual, open science},
location = {
},
series = {ETRA '25}
}



